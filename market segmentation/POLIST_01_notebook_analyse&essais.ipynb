{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %load_ext pycodestyle_magic\r\n",
    "# %pycodestyle_on\r\n",
    "# %pycodestyle_off\r\n",
    "\r\n",
    "# void"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%javascript\r\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\r\n",
    "    return false;\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.core.display import display, HTML\r\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import des librairies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "\r\n",
    "import pickle\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import plotly as plt\r\n",
    "import plotly.express as px\r\n",
    "import plotly.graph_objects as go\r\n",
    "\r\n",
    "import importlib\r\n",
    "\r\n",
    "import clean_lib as cl\r\n",
    "\r\n",
    "pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fonctions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_arity_cols(df_c1, df_c2):\r\n",
    "    \"\"\"\r\n",
    "    Renvoie des informations sur les relations entre\r\n",
    "    les deux colonnes de tables passées en arguments:\r\n",
    "    - a: 1 si la colonne df_c1 est toujours renseignée\r\n",
    "         0 sinon\r\n",
    "    - b_min: une valeur donnée de la colonne df_c1 est\r\n",
    "             liée au minimum à b_min valeurs différentes\r\n",
    "             de la colonne df_c2\r\n",
    "    - b_max: une valeur donnée de la colonne df_c1 est\r\n",
    "             liée au maximum à b_max valeurs différentes\r\n",
    "             de la colonne df_c2\r\n",
    "    - b_mean: une valeur donnée de la colonne df_c1 est\r\n",
    "              liée en moyenne à b_max valeurs différentes\r\n",
    "              de la colonne df_c2\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    df = pd.DataFrame({'from': df_c1, 'to': df_c2})\r\n",
    "\r\n",
    "    if df['from'].notna().sum() == 0:\r\n",
    "        (a, b) = (0, 0)\r\n",
    "    elif df.loc[df['from'].notna(), 'to'].notna().sum() == 0:\r\n",
    "        (a, b) = (0, 0)\r\n",
    "    else:\r\n",
    "        if df['from'].isna().sum() > 0:\r\n",
    "            a = 0\r\n",
    "        else:\r\n",
    "            a = 1\r\n",
    "\r\n",
    "        b_min = df[df['from'].notna()].groupby('from').nunique()['to'].min()\r\n",
    "        b_max = df[df['from'].notna()].groupby('from').nunique()['to'].max()\r\n",
    "        b_mean = df[df['from'].notna()].groupby('from').nunique()['to'].mean()\r\n",
    "\r\n",
    "    return(a, b_min, b_max, b_mean)\r\n",
    "\r\n",
    "\r\n",
    "def get_arity_df(df_):\r\n",
    "    \"\"\"\r\n",
    "    Renvoie dans un DataFrame,pour chaque paire de colonnes de df_,\r\n",
    "    les informations fournies par la fonction get_arity_cols()\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    arity = pd.DataFrame(columns=['key', 'from', 'to', 'arity', 'mean'])\r\n",
    "\r\n",
    "    for c1 in df_.columns:\r\n",
    "\r\n",
    "        key = df_[c1].nunique() == df_.shape[0]\r\n",
    "\r\n",
    "        for c2 in df_.columns:\r\n",
    "\r\n",
    "            if c1 == c2:\r\n",
    "                continue\r\n",
    "\r\n",
    "            df_c1 = df_[c1]\r\n",
    "            df_c2 = df_[c2]\r\n",
    "\r\n",
    "            a, b_min, b_max, b_mean = get_arity_cols(df_c1, df_c2)\r\n",
    "\r\n",
    "            arity_ = pd.DataFrame(\r\n",
    "                [[key, c1, c2, (a, b_min, b_max), round(b_mean, 2)]],\r\n",
    "                columns=['key', 'from', 'to', 'arity', 'mean']\r\n",
    "                )\r\n",
    "            arity = arity.append(arity_, ignore_index=True)\r\n",
    "\r\n",
    "    return(arity)\r\n",
    "\r\n",
    "\r\n",
    "def bizarre(df, c_from, c_to, max_to):\r\n",
    "    \"\"\"\r\n",
    "    Extrait du DataFrame renvoyé par get_arity_df()\r\n",
    "    les lignes pour lesquelles une valeur de la colonne c_from\r\n",
    "    a max_to correspondances dans c_to.\r\n",
    "    \"\"\"\r\n",
    "    group = df.groupby(c_from).nunique()[c_to]\r\n",
    "    ind = group[group == max_to].index\r\n",
    "    print(f\"Nombre de {c_from} ayant {max_to} {c_to}: {len(ind)}\")\r\n",
    "\r\n",
    "    return(df[df[c_from].isin(ind)].sort_values(by=c_from))\r\n",
    "\r\n",
    "\r\n",
    "def print_bizarre(df, c_from, c_to, max_to):\r\n",
    "    \"\"\"\r\n",
    "    Idem à la fonction bizarre(), mais n'affiche que le nombre\r\n",
    "    de lignes concernées sans les extraire. \r\n",
    "    \"\"\"\r\n",
    "    group = df.groupby(c_from).nunique()[c_to]\r\n",
    "    ind = group[group == max_to].index\r\n",
    "    print(f\"Nombre de {c_from} ayant {max_to} {c_to}: {len(ind)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fonction de feature engineering**  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def feat_eng(tabs, date=pd.to_datetime('01/01/2021')):\r\n",
    "    \"\"\"\r\n",
    "    Renvoie un DataFrame dont les colonnes sont les variables construites\r\n",
    "    à partir des données originales.\r\n",
    "    Ne prend que les informations portant sur des commandes effectuées\r\n",
    "    avant la date passée en argument.\r\n",
    "\r\n",
    "    Après sélection des variables utilisées pour le clustering,\r\n",
    "    certaines variables ont été commentées pour accélérer la fonction.\r\n",
    "    Elles sont indiquées par (*) ci-dessous.\r\n",
    "\r\n",
    "    Les variables contruites sont, pour chaque client identifié\r\n",
    "    par 'customer_unique_id':\r\n",
    "    - 'order_to_delivery': durée moyenne depuis la commande\r\n",
    "      jusqu'à la livraison\r\n",
    "    - (*)'order_purchasetime_stamp_max': date de la dernière commande\r\n",
    "    - 'last_order_week': numéro de semaine de la dernière commande\r\n",
    "    - (*)'order_purchasetime_stamp_min': date de la première commande\r\n",
    "    - (*)'delivery_delta': écart moyen entre la date de livraison prévue\r\n",
    "      et réelle\r\n",
    "    - (*)'most_freq_hour': heure de commande la plus fréquente\r\n",
    "    - (*)'most_freq_day': jour de commande le plus fréquent\r\n",
    "    - (*)'most_freq_week': semaine de commande la plus fréquente\r\n",
    "    - (*)'most_freq_month': mois de commande le plus fréquent\r\n",
    "    - 'n_orders': vaut 1 si le client a passé une seule commande et 2 \r\n",
    "       si le client a passé plusieurs commandes.\r\n",
    "    - 'payment_value': valeur moyenne des commandes du client\r\n",
    "    - 'review_score_mean: score moyen attribué par le client'\r\n",
    "    \"\"\"\r\n",
    "    custs, geo, oitems, opays, orevs, \\\r\n",
    "        orders, prods, sellers, translations = tabs\r\n",
    "\r\n",
    "    # Customers / Commandes\r\n",
    "    custs_ = custs.copy()\r\n",
    "    orders_ = orders.copy()\r\n",
    "    c_ = custs_.merge(orders_, how='left', on='customer_id')\r\n",
    "\r\n",
    "    # Filter orders prior to date\r\n",
    "    c_ = c_.loc[c_['order_purchase_timestamp'] < date]\r\n",
    "    orders_ = orders_.loc[orders_['order_purchase_timestamp'] < date]\r\n",
    "\r\n",
    "    # DECOMMENTER POUR ACTIVER\r\n",
    "    # Date \"d'aujourd'hui\", c'est à dire de la dernière commande réalisée\r\n",
    "    today = orders_['order_purchase_timestamp'].max()\r\n",
    "\r\n",
    "#     # DECOMMENTER POUR ACTIVER\r\n",
    "#     # Nombre de commande dans chacun des order status\r\n",
    "#     orders_status = orders_.groupby('order_status')['order_id'].count()\r\n",
    "\r\n",
    "    # Durée du processus order to delivery (to the client)\r\n",
    "    c_['order_to_delivery'] = \\\r\n",
    "        c_['order_delivered_customer_date'] - c_['order_purchase_timestamp']\r\n",
    "    # Filter - Grouper - Aggréger - Selectionner - Merger - Convertir\r\n",
    "    to_agg = c_[c_['order_to_delivery'].notna()]\r\n",
    "    to_agg = to_agg.groupby('customer_unique_id')\r\n",
    "    to_agg = to_agg['order_to_delivery'].mean(numeric_only=False)\r\n",
    "    df_feat = pd.DataFrame(to_agg)\r\n",
    "    df_feat['order_to_delivery'] = df_feat['order_to_delivery'].dt.days\r\n",
    "    df_feat['order_to_delivery'] = df_feat.to_numpy()\r\n",
    "\r\n",
    "    # DECOMMENTER POUR ACTIVER\r\n",
    "    # Temps depuis la dernière commande par le client\r\n",
    "    # Filter - Grouper - Aggréger - Selectionner - Merger - Convertir\r\n",
    "    to_agg = c_.groupby('customer_unique_id')\r\n",
    "    to_agg = to_agg['order_purchase_timestamp'].max()\r\n",
    "    to_agg = today - to_agg\r\n",
    "    df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "    columns = {'order_purchase_timestamp': 'order_purchasetime_stamp_max'}\r\n",
    "    df_feat = df_feat.rename(columns=columns)\r\n",
    "    feat = df_feat['order_purchasetime_stamp_max'].dt.days\r\n",
    "    feat = feat.to_numpy()\r\n",
    "    df_feat['order_purchasetime_stamp_max'] = feat\r\n",
    "\r\n",
    "    # Semaine de la dernière commande par le client\r\n",
    "    # Filter - Grouper - Aggréger - Selectionner - Merger - Convertir\r\n",
    "    custs_ = custs.copy()\r\n",
    "    orders_ = orders.copy()\r\n",
    "    c_ = custs_.merge(orders_, how='left', on='customer_id')\r\n",
    "    to_agg = c_.groupby('customer_unique_id')\r\n",
    "    to_agg = to_agg['order_purchase_timestamp'].max().dt.week\r\n",
    "    df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "    columns = {'order_purchase_timestamp': 'last_order_week'}\r\n",
    "    df_feat = df_feat.rename(columns=columns)\r\n",
    "\r\n",
    "#     # DECOMMENTER POUR ACTIVER\r\n",
    "#     # Temps depuis la première commande par client\r\n",
    "#     to_agg = c_.groupby('customer_unique_id')\r\n",
    "#     to_agg = to_agg['order_purchase_timestamp'].min()\r\n",
    "#     to_agg = today - to_agg\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "#     columns = {'order_purchase_timestamp': 'order_purchasetime_stamp_min'}\r\n",
    "#     df_feat = df_feat.rename(columns=columns)\r\n",
    "#     feat = df_feat['order_purchasetime_stamp_min'].dt.days\r\n",
    "#     feat = feat.to_numpy()\r\n",
    "#     df_feat['order_purchasetime_stamp_min'] = feat\r\n",
    "\r\n",
    "#     # DECOMMENTER POUR ACTIVER\r\n",
    "#     # Date de livraison - Date de livraison annoncée\r\n",
    "#     c_['delivery_delta'] = c_['order_delivered_customer_date'] \\\r\n",
    "#         - c_['order_estimated_delivery_date']\r\n",
    "#     to_agg = c_[c_['delivery_delta'].notna()]\r\n",
    "#     to_agg = to_agg.groupby('customer_unique_id')\r\n",
    "#     to_agg = to_agg['delivery_delta'].mean(numeric_only=False)\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "#     feat = df_feat['delivery_delta'] = df_feat['delivery_delta'].dt.days\r\n",
    "#     feat = feat.to_numpy()\r\n",
    "#     df_feat['delivery_delta'] = feat\r\n",
    "\r\n",
    "#     # DECOMMENTER POUR ACTIVER\r\n",
    "#     # Périodes de commande les plus fréquentes\r\n",
    "#     def agg_mode(x):\r\n",
    "#         return(x.mode()[0])\r\n",
    "#     c_['most_freq_hour'] = c_['order_purchase_timestamp'].dt.hour\r\n",
    "#     to_agg = c_.groupby('customer_unique_id')['most_freq_hour'].agg(agg_mode)\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "#     c_['most_freq_day'] = c_['order_purchase_timestamp'].dt.dayofweek\r\n",
    "#     to_agg = c_.groupby('customer_unique_id')['most_freq_day'].agg(agg_mode)\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "#     c_['most_freq_week'] = c_['order_purchase_timestamp'].dt.week\r\n",
    "#     to_agg = c_.groupby('customer_unique_id')['most_freq_week'].agg(agg_mode)\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "#     c_['most_freq_month'] = c_['order_purchase_timestamp'].dt.month\r\n",
    "#     to_agg = c_.groupby('customer_unique_id')['most_freq_month']\r\n",
    "#     to_agg = to_agg.agg(agg_mode)\r\n",
    "#     df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "\r\n",
    "    # Nombre de commandes par client au total\r\n",
    "    to_agg = c_.groupby('customer_unique_id')['order_id'].count()\r\n",
    "    df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "    df_feat = df_feat.rename(columns={'order_id': 'n_orders'})\r\n",
    "    # Si supérieur ou égal à 2 commandes, indiquer 2\r\n",
    "    df_feat.loc[df_feat['n_orders'] >= 2, 'n_orders'] = 2\r\n",
    "\r\n",
    "    # Price and Payments\r\n",
    "    to_agg = oitems.groupby('order_id')['price'].sum()\r\n",
    "    c_ = c_.merge(to_agg, how='left', on='order_id')\r\n",
    "    to_agg = oitems.groupby('order_id')['freight_value'].sum()\r\n",
    "    c_ = c_.merge(to_agg, how='left', on='order_id')\r\n",
    "    c_['payment_value_1'] = c_['price'] + c_['freight_value']\r\n",
    "    to_agg = opays.groupby('order_id')['payment_value'].sum()\r\n",
    "    c_ = c_.merge(to_agg, how='left', on='order_id')\r\n",
    "    c_['delta_price_pay'] = round((c_['payment_value_1']\r\n",
    "                                   - c_['payment_value']))\r\n",
    "\r\n",
    "    # Merge df_clustering\r\n",
    "    to_agg = c_.groupby('customer_unique_id')['payment_value'].mean()\r\n",
    "    df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "    # NB : des clients ont un solde non défini quand la commande n'a pas abouti\r\n",
    "    # On remplace les np.nan par zero\r\n",
    "    ind = df_feat.loc[:, 'payment_value'].isna()\r\n",
    "    df_feat.loc[ind, 'payment_value'] = 0\r\n",
    "\r\n",
    "    # Review score\r\n",
    "    to_agg = orevs.groupby('order_id')['review_score'].mean()\r\n",
    "    c_ = c_.merge(to_agg, how='left', on='order_id')\r\n",
    "    c_ = c_.rename(columns={'review_score': 'review_score_mean'})\r\n",
    "    to_agg = c_.groupby('customer_unique_id')['review_score_mean'].mean()\r\n",
    "    df_feat = df_feat.merge(to_agg, how='left', on='customer_unique_id')\r\n",
    "\r\n",
    "    return(df_feat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fonction de preprocessing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(df):\r\n",
    "    \"\"\"\r\n",
    "    Fonction de normalisation et centrage.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "    df_preprocess = df.copy()\r\n",
    "    sc = StandardScaler()\r\n",
    "    df_preprocess[:] = sc.fit_transform(df_preprocess)\r\n",
    "\r\n",
    "    return(df_preprocess)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fonction de clustering - KMeans**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_KMeans(df, cols, clusters_range, file_name, simu=False):\r\n",
    "    \"\"\"\r\n",
    "    Si simu est True:\r\n",
    "        Calcule le K-Means et le score de silhouette sur le DataFrame df\r\n",
    "        pour différents k pris dans clusters_range. Sauvegarde les scores\r\n",
    "        de silhouette dans le fichier file_name et renvoie la liste de ces\r\n",
    "        scores.\r\n",
    "\r\n",
    "    Sinon:\r\n",
    "        Charge les scores de silhouette depuis le fichier file_name.\r\n",
    "        Renvoie la liste des scores de silhouette.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    from sklearn.cluster import KMeans\r\n",
    "    from sklearn.metrics import silhouette_score\r\n",
    "\r\n",
    "    if simu:\r\n",
    "        silhouette = []\r\n",
    "        for i, n_clusters in enumerate(cluster_range):\r\n",
    "            print(f\"Simu {i+1} / {len(cluster_range)} en cours\")\r\n",
    "            km = KMeans(n_clusters=n_clusters)\r\n",
    "            km.fit(df.loc[:, cols])\r\n",
    "            silhouette.append(silhouette_score(df.loc[:, cols], km.labels_))\r\n",
    "\r\n",
    "        with open(file_name, 'wb') as f:\r\n",
    "            pickle.dump(silhouette, f, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "            print(f\"\\nTables dumpées dans un fichier pickle\")\r\n",
    "    else:\r\n",
    "        silhouette = pickle.load(open(file_name, \"rb\"))\r\n",
    "        print(f\"\\nSilhouettes importées depuis le fichier pickle\")\r\n",
    "\r\n",
    "    return(silhouette)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fonction de clustering - DBSCAN - Scan des densités du jeu de données**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_density(tabs, cols, eps, compute_db_params=False):\r\n",
    "    \"\"\"\r\n",
    "    Entrées:\r\n",
    "    tabs: la liste des tables de données\r\n",
    "    cols: les variables issues du features engineering\r\n",
    "        auxquelles on s'intéresse\r\n",
    "    eps: la liste des epsilons caractérisant le rayon d'une sphère autour\r\n",
    "        d'un point dans laquelle on va rechercher le nombre de voisins\r\n",
    "    compute_db_params: calculer les résultats si True et\r\n",
    "        les charger depuis un fichier sinon.\r\n",
    "\r\n",
    "    Si compute_db_params est True:\r\n",
    "        - Construit les variables du feature enginering dans df\r\n",
    "        - Sélectionne les variables df[cols]\r\n",
    "        - Centre et normalise df[cols]\r\n",
    "        - Calcule pour chaque point\r\n",
    "            1) La distance au plus proche voisin\r\n",
    "            2) Pour chaque epsilon de eps, le nombre de voisins du point\r\n",
    "               dans la sphère de rayon epsilon\r\n",
    "        - Sauvegarde les résultats dans un fichier\r\n",
    "        - Renvoie les résultats sous forme de DataFrame\r\n",
    "\r\n",
    "    Sinon:\r\n",
    "        - Charge les résultats à partir d'un fichier\r\n",
    "        - Renvoie les résultats sous forme de DataFrame\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    import numpy.linalg as LA\r\n",
    "    if compute_db_params:\r\n",
    "\r\n",
    "        # Construction des variables\r\n",
    "        df = feat_eng(tabs, date='06/11/2020')\r\n",
    "\r\n",
    "        # Nombre d'individus\r\n",
    "        n = len(df)\r\n",
    "\r\n",
    "        # Selection des variables\r\n",
    "        df = df[cols]\r\n",
    "\r\n",
    "        # Preprocessing et mise en forme\r\n",
    "        df = preprocess(df)\r\n",
    "        df = np.array(df)\r\n",
    "\r\n",
    "        # Calcul de la distance au plus proche voisin pour chaque individu\r\n",
    "        db_cols = ['dist_min']\r\n",
    "        db_cols.extend([str(round(e, 1)) for e in eps])\r\n",
    "        db = pd.DataFrame(columns=db_cols)\r\n",
    "\r\n",
    "        for i in range(len(df)):\r\n",
    "            if i % 10000 == 0:\r\n",
    "                print(i)\r\n",
    "            v = df[i, :].reshape(1, -1)\r\n",
    "            norm = LA.norm(df - v, axis=1)\r\n",
    "            norm = np.delete(norm, i)\r\n",
    "            norm = np.sqrt(norm)\r\n",
    "            out = [norm.min()]\r\n",
    "            out.extend([(norm < eps).sum() for eps in eps])\r\n",
    "            db.loc[i] = out\r\n",
    "\r\n",
    "        with open('db.p', 'wb') as f:\r\n",
    "            pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "            print(f\"\\nParamètres DB dumpées dans un fichier pickle\")\r\n",
    "\r\n",
    "    else:\r\n",
    "        db = pickle.load(open('db.p', \"rb\"))\r\n",
    "        print(f\"\\nParamètres DB importés depuis le fichier pickle\")\r\n",
    "\r\n",
    "    return(db)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paramétrage du notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Faut-il importer les données d'origines (True)\r\n",
    "# ou les données retravaillées (False)\r\n",
    "# Les données retravaillées contiennent les informations\r\n",
    "# sur les relations entre colonnes des tables\r\n",
    "do_import = False\r\n",
    "\r\n",
    "# Faut-il explorer les relations entre colonnes des tables ?\r\n",
    "explore_arity = True\r\n",
    "\r\n",
    "# Faut-il explorer les variables des tables d'origine ?\r\n",
    "explore_tables = True\r\n",
    "\r\n",
    "# Faut-il explorer les variables construites ?\r\n",
    "explore_features = True\r\n",
    "\r\n",
    "# Faut-il calculer les informations de densité ?\r\n",
    "compute_db_params = False\r\n",
    "\r\n",
    "# Paramètres d'affichage\r\n",
    "pd.set_option('max_r', 20)\r\n",
    "pd.options.display.max_rows = 999"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import des données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If do_import is True, datasets are uploaded and stored in a dictionnary\r\n",
    "if do_import:\r\n",
    "    custs = pd.read_csv(\"Data/olist_customers_dataset.csv\")\r\n",
    "    geo = pd.read_csv(\"Data/olist_geolocation_dataset.csv\")\r\n",
    "    oitems = pd.read_csv(\"Data/olist_order_items_dataset.csv\",\r\n",
    "                         parse_dates=[4],\r\n",
    "                         infer_datetime_format=True)\r\n",
    "\r\n",
    "# If do_import is True, compute the relationships between tables' columns\r\n",
    "if do_import:\r\n",
    "    print(f\"\\nDébut du calcul des arités\")\r\n",
    "    for t in tables:\r\n",
    "        print(f\"Calcul de l'arité de {t}\")\r\n",
    "        tables[t]['arity'] = get_arity_df(tables[t]['df'])\r\n",
    "    print(f\"Fin du calcul des arités\")\r\n",
    "\r\n",
    "    # Store tables in a pickle file\r\n",
    "    with open('tables.p', 'wb') as f:\r\n",
    "        pickle.dump(tables, f, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "        print(f\"\\nTables dumpées dans un fichier pickle\")\r\n",
    "\r\n",
    "# If do_import is False, upload the tables from the pickel file\r\n",
    "if not do_import:\r\n",
    "    tables = pickle.load(open(\"tables.p\", \"rb\"))\r\n",
    "    print(f\"\\nTables importées depuis le fichier pickle\")\r\n",
    "\r\n",
    "    # Raccourcis vers les tables\r\n",
    "    custs = tables['customers']['df']\r\n",
    "    geo = tables['geographic location']['df']\r\n",
    "    oitems = tables['order items']['df']\r\n",
    "    opays = tables['order payments']['df']\r\n",
    "    orevs = tables['order reviews']['df']\r\n",
    "    orders = tables['orders']['df']\r\n",
    "    prods = tables['products']['df']\r\n",
    "    sellers = tables['sellers']['df']\r\n",
    "    translations = tables['translations']['df']\r\n",
    "\r\n",
    "tabs = [custs, geo, oitems, opays, orevs, orders, prods, sellers, translations]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploration des tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration des tables: liaisons entre colonnes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exemple d'exploration des liaisons entre colonnes** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ci-dessous, on montre un exemple de calcul de relations entre colonnes pour la table de clients.  \n",
    "\n",
    "Les colonnes ont la signification suivante:  \n",
    "- 'key': est-ce que la variable 'from' est une clé unique de la table ?  \n",
    "- 'arity' = (p,min,max)\n",
    "    - p: la variable 'from' est-elle toujours présente (1:True, 0: False)\n",
    "    - min: la variable 'from' est liée à au moins min variables 'to\n",
    "    - max: la variable 'from' est liée à au plus max variable 'to'\n",
    "    - 'mean': la variable 'from' est liée en moyenne à 'mean' variable 'to'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_arity:\r\n",
    "    t = tables['customers']['arity']\r\n",
    "else:\r\n",
    "    t = None\r\n",
    "t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bizarre - Exemple**  \n",
    "On voit par exemple à la ligne 10 qu'un 'customer_zip_code_prefix' peut avoir jusqu'à 3 correspondances 'customer_city'. On regarde alors avec la fonction bizarre() quels sont ces cas de figure. Il s'agit d'un cas isolé lié à une erreur de frappe et d'entrée."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_arity:\r\n",
    "    b = bizarre(tables['customers']['df'],\r\n",
    "                'customer_zip_code_prefix', 'customer_city', 3)\r\n",
    "else:\r\n",
    "    b = None\r\n",
    "b"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Points remarquables issus de l'exploration des liaisons entre colonnes**  \n",
    "Ci-dessous, les points remarquables issus de l'exploration des relations entre colonnes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_arity:\r\n",
    "    print(\"\\nCUSTOMERS\")\r\n",
    "    print((\"Un client unique peut être lié à plusieurs client_id \"\r\n",
    "           \"car le client_id identifie en fait une commande\"))\r\n",
    "    print(\"client_id est en fait redondant avec order_id\")\r\n",
    "    df = tables['customers']['df']\r\n",
    "    df = df.merge(tables['orders']['df'], how='outer', on='customer_id')\r\n",
    "    print(f\"Nombre de customer_id sans order_id correspondant: \\\r\n",
    "          {df['order_id'].isna().sum()}\")\r\n",
    "    print(f\"Nombre de order_id sans customer_id correspondant: \\\r\n",
    "          {df['customer_id'].isna().sum()}\")\r\n",
    "\r\n",
    "    print(\"\\nORDER ITEMS\")\r\n",
    "    print((\"Une commande contient plusieurs lignes de commande, \"\r\n",
    "           \"les order items\"))\r\n",
    "    print(\"La clé de la table est order_id + order_item_id\")\r\n",
    "    print((\"Chaque ligne de commande est décrite par un produit, \"\r\n",
    "           \"vendeur, une date limite d'expédition, un prix et des frais de port\"))\r\n",
    "\r\n",
    "    print(\"\\nPAIMENTS\")\r\n",
    "    print((\"Un client peut payer par plusieurs moyens, \"\r\n",
    "           \"dans ce cas une séquence de paiement est créée\"))\r\n",
    "    print((\"Cela explique qu'une commande puisse \"\r\n",
    "           \"être associée à plusieurs paiements\"))\r\n",
    "    print((\"Chaque paiement peut être payée \"\r\n",
    "           \"en plusieurs versements ...\"))\r\n",
    "    print((\"... OU chaque paiment doit être fait \"\r\n",
    "           \"nombre de versements fois [A confirmer]\"))\r\n",
    "    print_bizarre(tables['order payments']['df'],\r\n",
    "                  'order_id', 'payment_value', 25)\r\n",
    "\r\n",
    "    print(\"\\nREVUES\")\r\n",
    "    print(\"Une commande peut-être liée à plusieurs revues, pas choquant\")\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'order_id', 'review_id', 1)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'order_id', 'review_id', 2)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'order_id', 'review_id', 3)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'order_id', 'review_id', 4)\r\n",
    "\r\n",
    "    print(\"\\nUne revue peut-être liée à plusieurs commandes, plus surprenant\")\r\n",
    "    print(\"Ca semble être des revues identiques sur un groupe de commandes\")\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'review_id', 'order_id', 1)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'review_id', 'order_id', 2)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'review_id', 'order_id', 3)\r\n",
    "    print_bizarre(tables['order reviews']['df'], 'review_id', 'order_id', 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration des tables: valeurs uniques, taux de remplissage, type de données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On voit ci-dessous que les données sont propres. Il n'y a pas de valeurs manquantes sauf pour certaines commandes en cours, des revues sans titre ou sans commentaire et un groupe de 610 produits.  \n",
    "Cela n'affecte pas le clustering ultérieur."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_tables:\r\n",
    "    importlib.reload(cl)\r\n",
    "\r\n",
    "    for t in tables:\r\n",
    "        print(\"\\nTable: \", t)\r\n",
    "        var_info = cl.get_var_info(tables[t]['df'],\r\n",
    "                                   how_few=0,\r\n",
    "                                   what='variables')\r\n",
    "        tables[t]['variables'] = var_info\r\n",
    "        display(HTML(tables[t]['variables'].to_html()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration des tables: statistiques sur les colonnes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Distribution des dates de commande, dates de scoring et montants payés**  \n",
    "Ci-dessous les distributions des variables qui nous servent à construire les variables sur lesquelles on fait le clustering.  \n",
    "En particulier, on remarque que les données s'arrêtent en réalité fin août."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.histogram(tables['orders']['df'], x='order_purchase_timestamp', title='Distribution des dates de commande', nbins = 180)\r\n",
    "fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.histogram(tables['order reviews']['df'], x='review_creation_date', title='Distribution des dates de scoring')\r\n",
    "fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.histogram(tables['order payments']['df'], x='payment_value', title='Distribution des montants payés')\r\n",
    "fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration des tables: doublons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il n'y a pas de doublons dans les tables sauf dans celle des coordonnées géographiques qu'on n'utilise pas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_tables:\r\n",
    "    for t in tables:\r\n",
    "        print(f\"Doublons dans la table {t}: \\\r\n",
    "        {tables[t]['df'].duplicated(keep=False).sum()}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**On crée la table sur laquelle on va appliquer le clustering grace à la fonction feat_eng() définie plus haut**\n",
    "\n",
    "Rappel des variables crées:\n",
    "\n",
    "    \"\"\"\n",
    "    Les variables contruites sont, pour chaque client identifié\n",
    "    par 'customer_unique_id':\n",
    "    - 'order_to_delivery': durée moyenne depuis la commande\n",
    "      jusqu'à la livraison\n",
    "    - (*)'order_purchasetime_stamp_max': date de la dernière commande\n",
    "    - 'last_order_week': numéro de semaine de la dernière commande\n",
    "    - (*)'order_purchasetime_stamp_min': date de la première commande\n",
    "    - (*)'delivery_delta': écart moyen entre la date de livraison prévue\n",
    "      et réelle\n",
    "    - (*)'most_freq_hour': heure de commande la plus fréquente\n",
    "    - (*)'most_freq_day': jour de commande le plus fréquent\n",
    "    - (*)'most_freq_week': semaine de commande la plus fréquente\n",
    "    - (*)'most_freq_month': mois de commande le plus fréquent\n",
    "    - 'n_orders': vaut 1 si le client a passé une seule commande et 2 \n",
    "       si le client a passé plusieurs commandes.\n",
    "    - 'payment_value': valeur moyenne des commande du client\n",
    "    - 'review_score_mean: score moyen attribué par le client'\n",
    "    \"\"\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_clustering = feat_eng(tabs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ci-dessous, la table créée pour le clustering qui ne contient que les valeurs qu'on a retenu au final pour le clustering.  \n",
    "Les autres variables construites dans le feature engineering ont été by-passées pour accélérer la fonction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_clustering.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exploration des données du features engineering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taux de remplissage de la table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_features:\r\n",
    "    importlib.reload(cl)\r\n",
    "    var_info = cl.get_var_info(df_clustering, how_few=0, what='variables')\r\n",
    "var_info"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Statistiques des variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_features:\n",
    "    importlib.reload(cl)\n",
    "    var_info = cl.get_var_info(df_clustering, how_few=0, what='stats')\n",
    "var_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distribution des variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if explore_features:\n",
    "    for c in df_clustering.columns:\n",
    "        fig = px.histogram(df_clustering, x=c)\n",
    "        fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Calcul du score de silhouette de K-Means appliqué à différents jeux de variables**  \n",
    "\n",
    "Ci-dessous on calcule pour différents k et différents jeux de variables construites les scores de silhouette des clusterings obtenus par K-Means.  \n",
    "\n",
    "Les graphs 'silhouette 1 à 4' plus bas correspondent à  des clustering effectués sur différents jeux de variables.\n",
    "\n",
    "On a choisit des variables pour avoir des informations de temps, fréquence d'achat, montant payé et satisfaction.\n",
    "\n",
    "On a testé d'abord la configuration suivante:  \n",
    "1) 'order_purchasetime_stamp_max', 'n_orders', 'payment_value', 'review_score_mean'  \n",
    "\n",
    "Puis on a testé la configuration ci-dessous ou on a remplacé 'order_purchasetime_stamp_max' par 'most_freq_week':   \n",
    "2) 'most_freq_week', 'n_orders', 'payment_value', 'review_score_mean'  \n",
    "\n",
    "Puis on a recommencé en rajoutant aux configuration ci-dessus la variable 'delivery_delta':  \n",
    "3) 'order_purchasetime_stamp_max', 'n_orders', 'payment_value', 'review_score_mean', 'delivery_delta'  \n",
    "4) 'most_freq_week', 'n_orders', 'payment_value', 'review_score_mean', 'delivery_delta'  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Init\r\n",
    "cluster_range = range(2, 10)\r\n",
    "do_simu = False\r\n",
    "\r\n",
    "# Construction des variables\r\n",
    "df = feat_eng(tabs, date='06/11/2020')\r\n",
    "\r\n",
    "# Preprocessing et mise en forme\r\n",
    "df = preprocess(df)\r\n",
    "\r\n",
    "cols = ['order_purchasetime_stamp_max',\r\n",
    "        'n_orders',\r\n",
    "        'payment_value',\r\n",
    "        'review_score_mean', ]\r\n",
    "silhouette1 = compute_KMeans(df, cols, cluster_range,\r\n",
    "                             'kmeans1.p', simu=do_simu)\r\n",
    "\r\n",
    "cols = ['most_freq_week',\r\n",
    "        'n_orders',\r\n",
    "        'payment_value',\r\n",
    "        'review_score_mean', ]\r\n",
    "silhouette2 = compute_KMeans(df, cols, cluster_range,\r\n",
    "                             'kmeans2.p', simu=do_simu)\r\n",
    "\r\n",
    "cols = ['order_purchasetime_stamp_max',\r\n",
    "        'n_orders', 'payment_value',\r\n",
    "        'review_score_mean',\r\n",
    "        'delivery_delta']\r\n",
    "silhouette3 = compute_KMeans(df, cols, cluster_range,\r\n",
    "                             'kmeans3.p', simu=do_simu)\r\n",
    "\r\n",
    "cols = ['most_freq_week',\r\n",
    "        'n_orders',\r\n",
    "        'payment_value',\r\n",
    "        'review_score_mean',\r\n",
    "        'delivery_delta']\r\n",
    "silhouette4 = compute_KMeans(df, cols, cluster_range,\r\n",
    "                             'kmeans4.p', simu=do_simu)\r\n",
    "\r\n",
    "silhouettes = np.array([silhouette1, silhouette2, silhouette3, silhouette4])\r\n",
    "columns = ['silhouette1', 'silhouette2', 'silhouette3', 'silhouette4']\r\n",
    "silhouettes = pd.DataFrame(silhouettes.T,\r\n",
    "                           columns=columns,\r\n",
    "                           index=cluster_range)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Graph des silhouettes**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explication des configuration silhouette 1 à 4 à la cellule précédente.  \n",
    "\n",
    "**Les configurations 1 et 2 sont meilleures.**  \n",
    "**Le meilleur score est atteint pour 5 ou 6 clusters, on choisit de prendre 5 clusters pour faciliter l'interprétation**  \n",
    "\n",
    "La configuration 2 donne un score encore meilleur pour 2 clusters en séparant très bien les clients qui achètent plusieurs fois des autres, mais cette segmentation à 2 classes est trop pauvre et on l'écarte."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title = 'K-Means: Silouhettes en fonction de k pour différentes configurations'\r\n",
    "labels = {'index': 'Nombre de clusters',\r\n",
    "          'value': 'Score de silhouette'}\r\n",
    "px.line(silhouettes,\r\n",
    "        title=title,\r\n",
    "       labels = labels)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Représentation des classes du K-Means clustering pour la configuration 2 et 5 classes**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "# Init\r\n",
    "cols = ['last_order_week', 'n_orders', 'review_score_mean', 'payment_value', ]\r\n",
    "\r\n",
    "# Construction des variables\r\n",
    "df = feat_eng(tabs, date='06/11/2020')\r\n",
    "\r\n",
    "# Preprocessing et mise en forme\r\n",
    "df_ = preprocess(df)\r\n",
    "\r\n",
    "km = KMeans(n_clusters=5)\r\n",
    "km.fit(df_.loc[:, cols])\r\n",
    "df['label'] = km.labels_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Nombre de clients par cluster\r\n",
    "title = 'Histogramme de la population de clients par classe'\r\n",
    "px.histogram(df, x='label',\r\n",
    "             title=title,\r\n",
    "            labels=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Interprétation des classes**  \n",
    "Pour interpréter les classes on représente ci-dessous la distribution des variables en fonction des classes.  \n",
    "\n",
    "La majorité des clients sont des clients satisfaits qui font un seul achat de faible valeur en début ou en fin d’année.  \n",
    "Certains clients se distinguent par le fait qu’ils:  \n",
    "- commandent plusieurs fois\n",
    "- ou, sont insatisfaits\n",
    "- ou, ont un panier moyen plus élevé\n",
    "  \n",
    "Attention: il y a 15 000 clients insatisfaits ! Les clients qui paient plus ou achètent plus souvent sont aussi plus exigeants.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for var in [c for c in cols if c != 'label']:\r\n",
    "    fig = go.Figure()\r\n",
    "    fig.add_trace(go.Box(x=df['label'], y=df[var], boxmean=True))\r\n",
    "    fig.update_layout(title=str(var))\r\n",
    "    fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Calcul de la dérive dans le temps du modèle de K-Means clustering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans le code ci-dessous, on mesure comment un modèle fourni à une date donnée à un client dérive au cours du temps.  \n",
    "La mesure utilisée est le 'Adjusted Rand Index'.  \n",
    "A travers ce score, on compare le modèle fourni à des modèles recalculés à différentes dates postérieures à la date de fourniture du modèle."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import adjusted_rand_score\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "# import warnings filter\r\n",
    "from warnings import simplefilter\r\n",
    "# ignore all future warnings\r\n",
    "simplefilter(action='ignore', category=FutureWarning)\r\n",
    "\r\n",
    "# Init\r\n",
    "n_clusters = 5\r\n",
    "load_from_pickel = True\r\n",
    "cols = ['last_order_week', 'n_orders', 'review_score_mean', 'payment_value', ]\r\n",
    "\r\n",
    "start_dates = pd.date_range('31/12/2016', periods=7, freq='3M')\r\n",
    "simus = {}\r\n",
    "\r\n",
    "if not load_from_pickel:\r\n",
    "    for start_date in start_dates:\r\n",
    "\r\n",
    "        periods = pd.date_range(start_date, periods=24, freq='W')\r\n",
    "        print(f\"\\nModèle livré le = {periods[0]}\")\r\n",
    "\r\n",
    "        # Modèle de référence pour la période commençant à periods[0]\r\n",
    "        df = feat_eng(tabs, date=periods[0])\r\n",
    "        df = df[cols]\r\n",
    "        sc = StandardScaler()\r\n",
    "        df[:] = sc.fit_transform(df)\r\n",
    "        km = KMeans(n_clusters=n_clusters)\r\n",
    "        km.fit(df)\r\n",
    "\r\n",
    "        x = []\r\n",
    "        y = []\r\n",
    "        means = []\r\n",
    "        stds = []\r\n",
    "\r\n",
    "        for date in periods:\r\n",
    "\r\n",
    "            if date > pd.to_datetime('2018-10-01'):\r\n",
    "                continue\r\n",
    "\r\n",
    "            df_ = feat_eng(tabs, date=date)\r\n",
    "            df_ = df_[cols]\r\n",
    "            \r\n",
    "#             means.append(df_.mean(axis=0))\r\n",
    "#             stds.append(df_.std(axis=0))\r\n",
    "            \r\n",
    "            # Modèle recalculé pour comparer au modèle de référence\r\n",
    "            # et appliqué aux données\r\n",
    "            df_true = df_.copy()\r\n",
    "            sc_true = StandardScaler()\r\n",
    "            df_true[:] = sc_true.fit_transform(df_true)\r\n",
    "            km_true = KMeans(n_clusters=n_clusters)\r\n",
    "            km_true.fit(df_true)\r\n",
    "            labels_true = km_true.labels_\r\n",
    "\r\n",
    "            # Modèle de référence appliqué aux données\r\n",
    "            df_predict = df_.copy()\r\n",
    "            df_predict[:] = sc.transform(df_predict)\r\n",
    "            labels_predict = km.predict(df_predict)\r\n",
    "            \r\n",
    "#             # Calcul de la dérive des centres de clusters\r\n",
    "#             centers_true = km_true.cluster_centers_\r\n",
    "#             centers_predict = km.cluster_centers_\r\n",
    "            \r\n",
    "#             dist = np.zeros((n_clusters, n_clusters))\r\n",
    "#             for i in range(n_clusters):\r\n",
    "#                 d = centers_true - (centers_predict[i,:].reshape(1,centers_predict.shape[1]))\r\n",
    "#                 d = d**2\r\n",
    "#                 d = d.sum(axis=1)\r\n",
    "#                 dist[:,i] = np.sqrt(d)\r\n",
    "                \r\n",
    "            ari = adjusted_rand_score(labels_true, labels_predict)\r\n",
    "    \r\n",
    "#             print(np.round(dist,2))\r\n",
    "            print(f\"Score le = {date}: {ari}\")\r\n",
    "    \r\n",
    "            if ari < 0.8:\r\n",
    "                break\r\n",
    "\r\n",
    "            x.append(date)\r\n",
    "            y.append(ari)\r\n",
    "        \r\n",
    "        simus[str(start_date)] = [x, y, means, stds]\r\n",
    "\r\n",
    "    with open('derive.p', 'wb') as f:\r\n",
    "        pickle.dump(simus, f, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "        print(f\"\\nDérives dumpées dans un fichier pickle\")\r\n",
    "else:\r\n",
    "    simus = pickle.load(open('derive.p', \"rb\"))\r\n",
    "    print(f\"\\nDérives importées depuis le fichier pickle\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Graph de la dérive dans le temps du modèle de K-Means clustering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns = ('seuil',\r\n",
    "           'Mise à disposition du modèle',\r\n",
    "           'Robustesse en nombre de semaines')\r\n",
    "\r\n",
    "derive = pd.DataFrame(columns=columns)\r\n",
    "\r\n",
    "for seuil in [0.95, 0.90, 0.85]:\r\n",
    "    s = []\r\n",
    "    x = []\r\n",
    "    y = []\r\n",
    "    for start in simus:\r\n",
    "        x.append(start)\r\n",
    "        sup_seuil = np.append((np.array(simus[start][1]) > seuil), False)\r\n",
    "        y.append(np.argmin(sup_seuil))\r\n",
    "        s.append(seuil)\r\n",
    "    derive_tmp = np.array([s, x, y]).T\r\n",
    "    derive_tmp = pd.DataFrame(derive_tmp, columns=columns)\r\n",
    "    derive = pd.concat([derive, derive_tmp])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "px.bar(derive,\r\n",
    "       x='Mise à disposition du modèle',\r\n",
    "       y='Robustesse en nombre de semaines',\r\n",
    "       color='seuil',\r\n",
    "       barmode='group',\r\n",
    "       title=\"Robustesse du modèle K-Means\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Le modèle de référence reste de bonne qualité (score entre 0.8 et 0.9) si il est remis à jour toutes les 3 semaines**  \n",
    "Au démarrage, la robustesse n'est que de deux semaines car le nombre de clients est faible et non représentatif de la population ultérieure."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DBSCAN  \n",
    "On teste ici si l'algorithme DBScan, basé sur l'exploitation de la densité du jeu de données est pertinent. On va constater que DBScan n'est pas pertinent pour le jeu de données car les points sont répartis dans l'espace selon des densités non homogènes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pour paramétrer l'algorithme, on calcule d'abord des informations de densité sur le jeu de données:**  \n",
    "Pour chaque point, (c'est à dire chaque client):\n",
    "- La distance au plus proche voisin\n",
    "- Le nombre de voisins à une distance inférieure à r pour r dans 0.1 - 2.0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eps = np.linspace(0.1, 2, 20)\r\n",
    "cols = ['last_order_week', 'n_orders', 'review_score_mean', 'payment_value', ]\r\n",
    "\r\n",
    "db = compute_density(tabs, cols, eps, compute_db_params=False)\r\n",
    "db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Choix de la résolution du clustering**  \n",
    "Pour trouver un epsilon cohérent de la densité du jeu de données, on trace la courbe de la distance au plus proche voisin de chaque point. \n",
    "(Les points sont classés par distance croissante)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "db_min = db['dist_min'].sort_values()\r\n",
    "title = 'Distance au plus proche voisin de chaque client'\r\n",
    "labels = {'index': 'Client', 'value': 'Distance au plus proche voisin'}\r\n",
    "px.line(db_min.to_numpy(),\r\n",
    "        title=title,\r\n",
    "        labels=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le coude de la courbe indique la zonne dans laquelle choisir un epsilon cohérent de la densité de jeu de données.  \n",
    "Ici: **on choisit 0.3** qui permet d'avoir suffisamment de clusters (pour des epsilons plus grands, les clusters sont agglomérés les uns aux autres (en d'autres termes, on perd trop en résolution)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Choix de la densité minimale de clustering**  \n",
    "Pour le epsilon retenu, on trace la courbe du nombre de voisins à moins de r = 0.3 de chaque client  \n",
    "C'est une aide au choix de min_samples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "db_min = db['0.3'].sort_values(ascending=False)\r\n",
    "title = 'Nombres de voisins à moins de r = 0.3 de chaque client'\r\n",
    "labels = {'index': 'Client', 'value': 'Nombres de voisins à moins de r = 0.3'}\r\n",
    "px.line(db_min.to_numpy(), title=title, labels = labels)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Par essai-erreurs on choisit min_samples de plus en plus grand pour diminuer le nombre de clusters obtenus en surveillant le nombre de clients considérés comme du bruit par l'algorithme.   \n",
    "Avec epsilon = 0.3, **on choisit min_samples = 80** pour obtenir 6 clusters en plus des points considérés comme du 'bruit'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import DBSCAN\r\n",
    "\r\n",
    "# Init\r\n",
    "cols = ['last_order_week', 'n_orders', 'review_score_mean', 'payment_value', ]\r\n",
    "\r\n",
    "# Construction des variables\r\n",
    "df = feat_eng(tabs, date='06/11/2020')\r\n",
    "df = df[cols]\r\n",
    "\r\n",
    "# Preprocessing\r\n",
    "df_ = preprocess(df)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DBSCAN\r\n",
    "df_ = np.array(df_)\r\n",
    "eps = 0.3\r\n",
    "min_samples = 80\r\n",
    "dbs = DBSCAN(eps=eps, min_samples=min_samples, )\r\n",
    "dbs.fit(df_)\r\n",
    "print(f\"\\n: Simu: eps = {eps}, min_samples = {min_samples}\")\r\n",
    "print(f\"Noisy points: {(dbs.labels_ == -1).sum()}\")\r\n",
    "print(f\"Clusters: {dbs.labels_.max() + 1}\")\r\n",
    "df['labels'] = dbs.labels_\r\n",
    "clusters_size = [(df['labels'] == i).sum()\r\n",
    "                 for i in np.sort(df['labels'].unique())]\r\n",
    "print(f\"Taille des clusters: \\\r\n",
    "    {list(zip(np.sort(df['labels'].unique()),clusters_size))}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Représentation des classes de DBScan clustering pour epsilon = 0.3 et min_samples = 80**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Nombre de clients par cluster\r\n",
    "px.histogram(df, x='labels', title = 'Histogramme de la population de clients par classe')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Interprétation des classes**  \n",
    "Pour interpréter les classes on représente ci-dessous la distribution des variables en fonction des classes.  \n",
    "\n",
    "On remarque que sur les 6 classes identifiées par DBScan, 5 correspondent au score moyen attribué au client. Le score moyen découpe l'espace en hyperplans bien séparés les uns des autres (il n'y a que 5 notes possibles). Sur ces plans les autres variables qui ont plus de modalités sont plus denses et forment les clusters.  \n",
    "\n",
    "De la même façon, DBScan identifie un plan bien séparé concernant la variable qui indique si un client a acheté une fois ou plusieurs fois (2 modalités uniquement).  \n",
    "\n",
    "Par ailleurs les clients qui paient plus que les autres sont trop éloignés des autres et sont considérés comme du bruit par l'algorithme.\n",
    "\n",
    "**En raison de l'hétérogénéité de la densité des points, l'algorithme n'est pas adapté au problème**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for var in [c for c in cols]:\r\n",
    "    fig = go.Figure()\r\n",
    "    fig.add_trace(go.Box(x=df['labels'], y=df[var], boxmean=True))\r\n",
    "    fig.update_layout(title=str(var))\r\n",
    "    fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering hiérarchique"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On teste ici si l'algorithme de classification ascendante hiérarchique est pertinent. On va constater qu'avec la métrique adéquate, il produit des classes similaires à celle de K-Means. Toutefois, on lui préferera le K-Means qui est plus rapide."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**On applique l'algorithme à un sous-échantillon du jeu de données**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from sklearn.cluster import AgglomerativeClustering\r\n",
    "\r\n",
    "# Init\r\n",
    "cols = ['last_order_week', 'n_orders', 'review_score_mean', 'payment_value', ]\r\n",
    "\r\n",
    "# Construction des variables\r\n",
    "df = feat_eng(tabs, date='06/11/2020')\r\n",
    "df = df[cols]\r\n",
    "\r\n",
    "# Sampling\r\n",
    "random_state = 47\r\n",
    "n_population = len(df)\r\n",
    "n_sample = round(n_population * 0.4)\r\n",
    "samples = sample_without_replacement(n_population,\r\n",
    "                                     n_sample,\r\n",
    "                                     random_state=random_state)\r\n",
    "df = df.iloc[samples]\r\n",
    "\r\n",
    "# Preprocessing\r\n",
    "df_ = preprocess(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CAH\r\n",
    "df_ = np.array(df_)\r\n",
    "agglo = AgglomerativeClustering(distance_threshold=None,\r\n",
    "                                n_clusters=5,\r\n",
    "                                linkage='ward')\r\n",
    "agglo.fit(df_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Représentation des classes de la CAH  pour n_clusters = 5**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['labels'] = agglo.labels_\r\n",
    "\r\n",
    "# Nombre de clients par cluster\r\n",
    "px.histogram(df, x='labels', title = 'Histogramme de la population de clients par classe')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Interprétation des classes**  \n",
    "Ci-dessous, on constate qu'on retrouve des classes similaires à celle du clustering K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['labels'] = agglo.labels_\r\n",
    "for var in [c for c in cols]:\r\n",
    "    fig = go.Figure()\r\n",
    "    fig.add_trace(go.Box(x=df['labels'],\r\n",
    "                         y=df[var],\r\n",
    "                         boxmean=True))\r\n",
    "    fig.update_layout(title=str(var))\r\n",
    "    fig.show('notebook')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Annexes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #Rand Index calculation\r\n",
    "\r\n",
    "# # a: groupés dans true and predict\r\n",
    "# # d: séparés dans true et predict\r\n",
    "# # b: séparés dand true mais groupés dans predict\r\n",
    "# # c: groupés dans true mais séparés dans predict\r\n",
    "\r\n",
    "# a = float(0)\r\n",
    "# d = float(0)\r\n",
    "# b = float(0)\r\n",
    "# c = float(0)\r\n",
    "# n = len(labels_predict)\r\n",
    "\r\n",
    "# for i in range(n):\r\n",
    "#     grouped_true = (labels_true == labels_true[i])\r\n",
    "#     grouped_predict = (labels_predict == labels_predict[i])\r\n",
    "#     a = a + (grouped_true &  grouped_predict).sum() - 1\r\n",
    "#     d = d + ((~ grouped_true) & (~ grouped_predict)).sum()\r\n",
    "#     b = b + (grouped_true & (~ grouped_predict)).sum()\r\n",
    "#     c = c + ((~grouped_true) & grouped_predict).sum()\r\n",
    "\r\n",
    "# rand_index = (a+b) / (n*(n-1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}