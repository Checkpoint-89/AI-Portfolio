{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0cb177665c8d0483b6777e38b6ceeeb12d1b76da116ab2b793d52c554572199d3",
   "display_name": "Python 3.8.5 64-bit ('P9': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis and Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import librairies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "import pickle\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import plotly as plt\r\n",
    "import plotly.express as px\r\n",
    "\r\n",
    "# void"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data  \n",
    "Load in 'clicks' the interactions file clicks_agg.csv, which is an aggregation of the hourly aggregation files.  \n",
    "Load in 'metadata' the articles metadata file.  \n",
    "Load in 'embs' the articles embedding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_path = './data'\r\n",
    "clicks_dir = os.path.join(data_path, 'clicks')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata_file = os.path.join(data_path, 'articles_metadata.csv')\r\n",
    "clicks_file = os.path.join(data_path, 'clicks_agg.csv')\r\n",
    "articles_file = os.path.join(data_path, 'articles_embeddings.pickle')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata = pd.read_csv(metadata_file)\r\n",
    "clicks = pd.read_csv(clicks_file)\r\n",
    "with open(articles_file, mode='rb') as f:\r\n",
    "    embs = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Look at the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clicks.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clicks.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clicks.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of unique user_id, session_id, click_article_id, and article_id\n",
    "print(f\"Number of unique user_id: {clicks['user_id'].nunique()}\")\n",
    "print(f\"Number of unique session_id: {clicks['session_id'].nunique()}\")\n",
    "print(f\"Number of unique click_article_id: {clicks['click_article_id'].nunique()}\")\n",
    "print(f\"Number of unique article_id: {len(embs)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of sessions per user\n",
    "sessions_per_user = clicks.groupby(by='user_id')['session_id'].nunique()\n",
    "# px.histogram(sessions_per_user, range_x=[0,50], title='Number of sessions per user').show()\n",
    "\n",
    "# # Number of article per session\n",
    "articles_per_session = clicks.groupby(by='session_id')['click_article_id'].nunique()\n",
    "# px.histogram(articles_per_session, range_x=[0,15], title='Number of articles per session').show()\n",
    "\n",
    "# # Number of user per article\n",
    "users_per_article = clicks.groupby(by='click_article_id')['user_id'].nunique()\n",
    "# px.histogram(users_per_article, range_x=[0,50], title='Number of users per article', nbins=50000).show()\n",
    "\n",
    "# # Number of articles per user\n",
    "articles_per_user = clicks.groupby(by='user_id')['click_article_id'].nunique()\n",
    "# px.histogram(articles_per_user, range_x=[0,100], title='Number of articles per user', nbins=50000).show()\n",
    "\n",
    "print(f\"Sessions per user - max:{sessions_per_user.max()} - min:{sessions_per_user.min()} - mean:{sessions_per_user.mean()}\")\n",
    "print(f\"Articles per session - max:{articles_per_session.max()} - min:{articles_per_session.min()} - mean:{articles_per_session.mean()}\")\n",
    "print(f\"User per article - max:{users_per_article.max()} - min:{users_per_article.min()} - mean:{users_per_article.mean()}\")\n",
    "print(f\"Article per user - max:{articles_per_user.max()} - min:{articles_per_user.min()} - mean:{articles_per_user.mean()}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add some helpers data  \n",
    "In the interactions dataframe named 'clicks':\n",
    "- create columns to identify the last click of a given session  \n",
    "- join the column words_count of 'metadata' to 'click'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create user_session_id and its changes\n",
    "clicks = clicks.sort_values(['user_id', 'session_id', 'click_timestamp'])\n",
    "clicks = clicks.reset_index(drop=True)\n",
    "clicks['user_session_id'] = clicks['user_id'].astype(str) + '_' + clicks['session_id'].astype(str)\n",
    "clicks['user_id_change'] = clicks['user_id'].diff() != 0\n",
    "clicks['session_id_change'] = clicks['session_id'].diff() != 0\n",
    "clicks['user_session_id_change'] = clicks['user_id_change'] & clicks['session_id_change']\n",
    "\n",
    "clicks  = clicks.join(metadata[['article_id', 'words_count']],on='click_article_id', how='left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean the data  \n",
    "Some articles have 0 word. We remove the sessions containing such articles from the data. We check that the impact of that operation on the amount of data available is limited."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clean the data\n",
    "# Drop sessions with articles that have words_count = 0\n",
    "user_id_before = clicks['user_id'].nunique()\n",
    "session_id_before = clicks['session_id'].nunique()\n",
    "click_articles_id_before = clicks['click_article_id'].nunique()\n",
    "\n",
    "min_words_count_per_session = clicks.groupby(by='session_id')['words_count'].min()\n",
    "sessions_to_drop = min_words_count_per_session[min_words_count_per_session == 0].index\n",
    "indexes_to_drop = clicks[clicks['session_id'].isin(sessions_to_drop)].index\n",
    "clicks = clicks.drop(index=indexes_to_drop)\n",
    "\n",
    "print(f\"Number of unique user_id: before {user_id_before} => after {clicks['user_id'].nunique()}\")\n",
    "print(f\"Number of unique session_id: before {session_id_before} => after {clicks['session_id'].nunique()}\")\n",
    "print(f\"Number of unique click_article_id: before {click_articles_id_before} => after {clicks['click_article_id'].nunique()}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute an implicit rating of each article seen by a user (each interaction)\n",
    "\n",
    "The proxy we use for the rating is the time spent on each article normalized by the number of words in the article  \n",
    "\n",
    "1) It is computed as:  \n",
    "> time spent on the current article = timestamp of the click on the next article - timestamp of the click on the current article  \n",
    "> normalized time spent on the current article =  time spent on the current article / number of words of the article\n",
    "  \n",
    "2) This computation can be done for all articles within a session but the last one because it has no next article.  \n",
    "In that case the value is set to np.nan.  \n",
    "\n",
    "3) Due to the very long tail of the resulting distribution, we take its log. The log distribution still have a narrow pick and we further adjust the rating by compressing both sides of the peak around it.\n",
    "\n",
    "4) Finally the ratings are normalized on a scale of 0 to 5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1) Compute a proxy for the ratings\n",
    "\n",
    "starts = clicks['click_timestamp'].values\n",
    "starts = np.append(starts, starts[-1])\n",
    "stops = starts[1:]\n",
    "starts = starts[0:-1]\n",
    "clicks['article_deltatime'] = (stops-starts)/1000.\n",
    "\n",
    "last_session_click = clicks['session_id_change']\n",
    "last_session_click = last_session_click[1:].append(pd.Series([True]))\n",
    "clicks['last_session_click'] = last_session_click.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2) Set clicks where this computation is not possible to np.nan\n",
    "idx = clicks['last_session_click'] == True\n",
    "clicks.loc[idx, 'article_deltatime'] = np.nan\n",
    "\n",
    "clicks['score'] = clicks['article_deltatime'] / clicks['words_count']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3) Take the log\n",
    "clicks['rating'] = np.log10(clicks['score'] + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 4) Compress the distribution around its peak\n",
    "th = 0.05\n",
    "ratings_above_th = clicks.loc[clicks['rating']>th,'rating']\n",
    "clicks.loc[clicks['rating']>th,'rating'] = ratings_above_th / (ratings_above_th/th)**(0.8)\n",
    "\n",
    "ratings_below_th = clicks.loc[clicks['rating']<=th,'rating']\n",
    "clicks.loc[clicks['rating']<=th,'rating'] = ratings_below_th * (ratings_below_th/th)**(0.8)\n",
    "\n",
    "clicks['rating'] = clicks['rating']/clicks['rating'].max()*5\n",
    "\n",
    "px.histogram(clicks['rating'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clicks.to_csv('./data/clicks_enhanced.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clicks = clicks[['user_id', 'click_article_id', 'rating']]\n",
    "clicks = clicks.rename(columns={'user_id':'userID', 'click_article_id': 'itemID'})\n",
    "clicks.to_csv('./data/clicks_light.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Back up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Time spent on an article\n",
    "# px.histogram(clicks['article_timespan'], title = 'Time spent on an article')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# file_paths = glob.glob(os.path.join(clicks_dir,'*'))\n",
    "# clicks_agg_file = os.path.join(data_path, 'clicks_agg.csv')\n",
    "\n",
    "# with open(clicks_agg_file, mode='w') as agg_f:\n",
    "#     with open(file_paths[0]) as f:\n",
    "#         agg_f.write(f.read())\n",
    "#     for file_path in file_paths[1:]:\n",
    "#         with open(file_path) as f:\n",
    "#             agg_f.writelines(f.readlines()[1:])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}