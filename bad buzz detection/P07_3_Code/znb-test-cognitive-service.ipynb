{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('P7-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "107dea3cf51868a48003d659cb2f5935283e6d76b19b907a9175640cee3ed1c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\src\" not in sys.path:\n",
    "    sys.path.append(r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\src\")\n",
    "\n",
    "import importlib\n",
    "\n",
    "import config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import authenticate\n",
    "import api_sentiment_analysis\n",
    "import func\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "importlib.reload(config)\n",
    "importlib.reload(authenticate)\n",
    "importlib.reload(api_sentiment_analysis)\n",
    "importlib.reload(func)\n",
    "\n",
    "from authenticate import authenticate_client\n",
    "from api_sentiment_analysis import api_sentiment_analysis\n",
    "\n",
    "key = config.COGNITIVE_SERVICE_KEY\n",
    "endpoint = config.SENTIMENT_ANALYSIS_ENDPOINT\n",
    "\n",
    "data_path = \"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\data\\data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and sample the tweets\n",
    "df = pd.read_csv(data_path,  encoding=\"ISO-8859-1\", usecols=[0, 5], names=[\"label\",\"tweet\"])\n",
    "tweets, true_label = func.sample_tweets(df, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of results: 2000\n"
     ]
    }
   ],
   "source": [
    "# # Instantiate the client\n",
    "# client = authenticate_client(key, endpoint)\n",
    "\n",
    "# # Call the sentiment analysis API\n",
    "# # Batches of 9 tweets per call\n",
    "# results = []\n",
    "# n_batches = int(np.ceil(len(tweets)/10))\n",
    "# for i in range(n_batches):\n",
    "#     result = api_sentiment_analysis(client, tweets[i*10:(i+1)*10])\n",
    "#     results.extend(result)\n",
    "#     pass\n",
    "\n",
    "# print(f\"Length of results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save api predictions\n",
    "# with open(r\".\\api-results\\results.p\", 'wb') as f:\n",
    "#     pickle.dump(results, f)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of results: 2000\nTrue\n"
     ]
    }
   ],
   "source": [
    "# Load api predictions\n",
    "with open(r\".\\api-results\\results.p\", 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"Length of results: {len(results)}\")\n",
    "print(len(results)==len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of results: 2000\nTrue\n"
     ]
    }
   ],
   "source": [
    "X = [(results[i]['confidence_scores']['positive'],\n",
    "    results[i]['confidence_scores']['neutral'],\n",
    "    results[i]['confidence_scores']['negative']) \n",
    "    for i in range(len(results))]\n",
    "\n",
    "print(f\"Length of results: {len(X)}\")\n",
    "print(len(results)==len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score hors tweets prédits comme neutres\n% de score neutre: 0.2575\nScore sur les tweets hors neutres\nAccuracy score:  0.769023569023569\nF1 score:  0.7677725118483412\nAUC score:  0.7704365988264292\n\nScore global\n% de prédiction neutre: 0.0\nScore sur les tweets hors neutres\nAccuracy score:  0.731\nF1 score:  0.7485981308411215\nAUC score:  0.7310000000000001\n"
     ]
    }
   ],
   "source": [
    "# Décision simple sur la sortie de l'API\n",
    "trans = {'0': 1, '1':-1 , '2':0}\n",
    "\n",
    "##############################################################\n",
    "print(\"Score hors tweets prédits comme neutres\")\n",
    "\n",
    "predict = []\n",
    "for i in range(len(X)):\n",
    "    argmax= np.argmax(X[i])\n",
    "    predict.append(trans[str(argmax)])\n",
    "predict = np.array(predict)\n",
    "\n",
    "# Pourcentage de tweets labellisés 'neutre'\n",
    "print(f\"% de score neutre: {(predict==-1).sum() / len(predict) }\")\n",
    "\n",
    "# Score sur les tweets hors ceux labellisés neutres\n",
    "idx = (predict!=-1)\n",
    "pred = predict[idx]\n",
    "true_label_ = np.array(true_label)[idx]\n",
    "print(\"Score sur les tweets hors neutres\")\n",
    "print(\"Accuracy score: \",accuracy_score(true_label_, pred))\n",
    "print(\"F1 score: \",f1_score(true_label_, pred))\n",
    "print(\"AUC score: \",roc_auc_score(true_label_, pred))\n",
    "\n",
    "##############################################################\n",
    "print(\"\\nScore global\")\n",
    "\n",
    "predict = []\n",
    "for i in range(len(X)):\n",
    "    argmax= np.argmax((X[i][0], 0, X[i][2]))\n",
    "    predict.append(trans[str(argmax)])\n",
    "predict = np.array(predict)\n",
    "\n",
    "# Pourcentage de tweets labellisés 'neutre'\n",
    "print(f\"% de prédiction neutre: {(predict==-1).sum() / len(predict) }\")\n",
    "\n",
    "# Score\n",
    "idx = (predict!=-1)\n",
    "pred = predict[idx]\n",
    "true_label_ = np.array(true_label)[idx]\n",
    "print(\"Score sur les tweets hors neutres\")\n",
    "print(\"Accuracy score: \",accuracy_score(true_label_, pred))\n",
    "print(\"F1 score: \",f1_score(true_label_, pred))\n",
    "print(\"AUC score: \",roc_auc_score(true_label_, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}