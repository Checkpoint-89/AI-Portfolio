{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Train the model locally  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import librairies (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sys.path\n",
    "ROOT_DIR = r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\"\n",
    "SRC_DIR = ROOT_DIR + r\"\\src\"\n",
    "import sys\n",
    "sys.path.append(SRC_DIR)\n",
    "import config\n",
    "import pickle\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\cdiet\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\cdiet\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import load\n",
    "import preprocess\n",
    "import models\n",
    "importlib.reload(models)\n",
    "import simu\n",
    "import simu_framework\n",
    "import embeddings"
   ]
  },
  {
   "source": [
    "## Define files' paths (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = ROOT_DIR + r\"\\data\"\n",
    "EMBEDDING_DIR = ROOT_DIR + r\"\\embeddings\"\n",
    "DATA_FILE = ROOT_DIR + r\"\\data\\data.csv\"\n",
    "CHECKPOINT_FILE = ROOT_DIR + r\"\\checkpoints\\weights.best.hdf5\"\n",
    "model_name = config.model_name\n",
    "MODEL_FILE = f\"./models/model_{model_name}.p\"\n",
    "MODEL_PARAMS_FILE = f\"./models/model_params_{model_name}.p\""
   ]
  },
  {
   "source": [
    "## Define the parameters of the model (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model_params\n",
    "importlib.reload(config)\n",
    "\n",
    "model_params = dict()\n",
    "\n",
    "model_params['train_test_split'] = dict()\n",
    "model_params['train_test_split']['n_samples'] = config.n_samples\n",
    "model_params['train_test_split']['test_size'] = config.test_size\n",
    "model_params['train_test_split']['val_test_ratio'] = config.val_test_ratio\n",
    "\n",
    "model_params['preprocessing'] = dict()\n",
    "model_params['preprocessing']['stemming'] = config.stemming\n",
    "model_params['preprocessing']['lemmatization'] = config.lemmatization\n",
    "model_params['preprocessing']['pos_tag'] = config.pos_tag\n",
    "model_params['preprocessing']['max_df'] = config.max_df\n",
    "model_params['preprocessing']['min_df'] = config.min_df\n",
    "\n",
    "model_params['embedding'] = dict()\n",
    "model_params['embedding']['type'] = config.embedding_type\n",
    "model_params['embedding']['output_dim'] = config.embedding_output_dim\n",
    "model_params['embedding']['trainable'] = config.embedding_trainable\n",
    "\n",
    "model_params['model'] = dict()\n",
    "model_params['model']['type'] = config.model_type\n",
    "\n",
    "model_params['model_selection'] = dict()\n",
    "model_params['model_selection']['n_folds'] = config.n_folds\n",
    "model_params['model_selection']['epochs'] = config.epochs\n",
    "model_params['model_selection']['batch_size'] = config.batch_size\n",
    "\n",
    "model_params['model_selection']['learning_rate'] = config.learning_rate\n",
    "\n",
    "model_params['model_selection']['dropout_rate'] = config.dropout_rate\n",
    "model_params['model_selection']['l2_reg'] = config.l2_reg\n",
    "\n",
    "model_params['model_selection']['es_min_delta'] = config.early_stopping_min_delta\n",
    "model_params['model_selection']['es_patience'] = config.early_stopping_patience\n",
    "model_params['model_selection']['lr_reduce_factor'] = config.lr_reduce_factor\n",
    "model_params['model_selection']['lr_reduce_min_delta'] = config.lr_reduce_min_delta\n",
    "model_params['model_selection']['lr_reduce_patience'] = config.lr_reduce_patience\n",
    "model_params['model_selection']['lr_reduce_min_lr'] = config.lr_reduce_min_lr\n",
    "\n",
    "model_params['paths'] = dict()\n",
    "model_params['paths']['checkpoint_file'] = CHECKPOINT_FILE\n",
    "model_params['paths']['model_file'] = MODEL_FILE\n",
    "model_params['paths']['model_params_file'] = MODEL_PARAMS_FILE"
   ]
  },
  {
   "source": [
    "## Load the model (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "****************************************\n",
      "Load the data\n",
      "****************************************\n",
      "Data loaded\n",
      "Original dataset size: (1600000, 2)\n",
      "\n",
      "****************************************\n",
      "Sample the data\n",
      "****************************************\n",
      "Data sampled\n",
      "Sampled dataset size: ((20000,), (20000,))\n",
      "\n",
      "****************************************\n",
      "Train and test split\n",
      "****************************************\n",
      "Train and test split done\n",
      "Train set size: ((19000,), (19000,))\n",
      "Test set size: ((1000,), (1000,))\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(load)\n",
    "corpus_train, corpus_test, y_train, y_test = load.load_and_split_docs(\n",
    "    DATA_FILE,\n",
    "    n_tweets=model_params['train_test_split']['n_samples'],\n",
    "    test_size = model_params['train_test_split']['test_size']\n",
    ")"
   ]
  },
  {
   "source": [
    "## Preprocess the data (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "****************************************\n",
      "Preprocess train set\n",
      "****************************************\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cdiet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cdiet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Tokenization of the train set done\n",
      "Vocabulary size after tokenization: 18516\n",
      "Vocabulary reduction done\n",
      "Vocabulary size after reduction: 1917\n",
      "Encoding of the train set done\n",
      "Padding of the train set done\n",
      "Train size: (19000, 19000)\n",
      "\n",
      "****************************************\n",
      "Preprocess test set\n",
      "****************************************\n",
      "Tokenization of the test set done\n",
      "Encoding of the test set done\n",
      "Padding of the test set done\n",
      "Test size: (1000, 1000)\n",
      "\n",
      "****************************************\n",
      "Split test set further into validation and test sets\n",
      "****************************************\n",
      "Split into dev and test set done\n",
      "Validation size: (500, 500)\n",
      "Test size: (500, 500)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(preprocess)\n",
    "\n",
    "# Preprocess the train set\n",
    "preprocessing_inputs= (corpus_train, y_train, model_params)\n",
    "tk_train_corpus, padded_corpus_train = preprocess.preprocess_train(*preprocessing_inputs)\n",
    "\n",
    "# Preprocessing the test set\n",
    "preprocessing_inputs = (corpus_test, y_test, model_params)\n",
    "padded_corpus_val, padded_corpus_test, y_val, y_test = preprocess.preprocess_test(*preprocessing_inputs)\n",
    "\n",
    "vocab_train = model_params['preprocessing']['vocab_train']\n",
    "tk_doc_max_len = model_params['preprocessing']['tk_doc_max_len']"
   ]
  },
  {
   "source": [
    "## Compute the embeddings (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(embeddings)\n",
    "# embeddings.compute_dummy_embedding(EMBEDDING_DIR, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(embeddings)\n",
    "# embeddings.load_glove_embedding(DATA_DIR, EMBEDDING_DIR, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(embeddings)\n",
    "# embeddings.compute_w2v_embedding(EMBEDDING_DIR, tk_train_corpus, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(embeddings)\n",
    "# embeddings.load_w2v_embedding(DATA_DIR, EMBEDDING_DIR, vocab_train)"
   ]
  },
  {
   "source": [
    "## Load the selected embedding (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n****************************************\nSelect and load the embedding\n****************************************\nSelected embedding: pretrained_glove\nOriginal embedding loaded\nEmbedding reduced to the words in the vocabulary\nShape of the weights: (1917, 200)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(embeddings)\n",
    "\n",
    "weights = embeddings.select_embedding(\n",
    "        EMBEDDING_DIR,\n",
    "        vocab=vocab_train,\n",
    "        embedding_type=model_params['embedding']['type']\n",
    "    )\n",
    "\n",
    "# Record model parameter\n",
    "model_params['embedding']['weights'] = weights\n",
    "model_params['embedding']['output_dim'] = weights.shape[1]"
   ]
  },
  {
   "source": [
    "## Train the model (local test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "****************************************\n",
      "Train model\n",
      "****************************************\n",
      "\n",
      "Configuration 1 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56370, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56370 to 0.54145, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54145\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54145\n",
      "\n",
      " Currently best mean val_loss: 0.54145348072052\n",
      "\n",
      " Currently best mean val_accuracy: 0.7379999756813049\n",
      "\n",
      "Configuration 2 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.0001}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57323, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57323 to 0.52999, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52999\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52999\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52999\n",
      "\n",
      " Currently best mean val_loss: 0.529994010925293\n",
      "\n",
      " Currently best mean val_accuracy: 0.7160000205039978\n",
      "\n",
      "Configuration 3 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.01}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53196, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53196\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53196\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53196\n",
      "\n",
      "Configuration 4 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 1.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61155, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61155 to 0.58864, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58864\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58864\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58864\n",
      "\n",
      "Configuration 5 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51955, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51955\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51955\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51955\n",
      "\n",
      " Currently best mean val_loss: 0.5195522904396057\n",
      "\n",
      " Currently best mean val_accuracy: 0.7400000095367432\n",
      "\n",
      "Configuration 6 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.0001}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52743, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52743\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52743\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52743\n",
      "\n",
      "Configuration 7 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.01}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53546, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53546 to 0.53537, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53537\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53537\n",
      "\n",
      "Configuration 8 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 1.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60842, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60842 to 0.59523, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59523\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59523 to 0.59264, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59264\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59264\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59264\n",
      "\n",
      "Configuration 9 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53215, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53215 to 0.53141, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53141\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53141\n",
      "\n",
      "Configuration 10 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.0001}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53376, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53376 to 0.53375, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53375\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53375\n",
      "\n",
      "Configuration 11 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.01}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51953, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51953\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51953\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51953\n",
      "\n",
      " Currently best mean val_loss: 0.519533097743988\n",
      "\n",
      " Currently best mean val_accuracy: 0.7319999933242798\n",
      "\n",
      "Configuration 12 / 12: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 1.0}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60200, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.60200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.60200\n",
      "\n",
      "****************************************\n",
      "Records of the training\n",
      "****************************************\n",
      "{'configs': {'0': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.0}, '1': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.0001}, '2': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 0.01}, '3': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.0, 'l2_reg': 1.0}, '4': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.0}, '5': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.0001}, '6': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 0.01}, '7': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l2_reg': 1.0}, '8': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.0}, '9': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.0001}, '10': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.01}, '11': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 1.0}}, 'mean_train_losses_history': {'0': array([0.53522325, 0.43507385, 0.38355127, 0.32280365, 0.26434195]), '1': array([0.53374147, 0.44115722, 0.38676494, 0.33400565, 0.27374649]), '2': array([0.53989124, 0.44859472, 0.40726188, 0.35755169]), '3': array([0.80635077, 0.5385648 , 0.52020305, 0.50532842, 0.48451012]), '4': array([0.53872323, 0.446513  , 0.39887887, 0.34545225]), '5': array([0.54025906, 0.44307771, 0.39663351, 0.34242263]), '6': array([0.53821033, 0.45343751, 0.4135969 , 0.37072706]), '7': array([0.78144395, 0.54358852, 0.51662666, 0.49835157, 0.48295355,\n",
      "       0.46562237, 0.44668776]), '8': array([0.55002773, 0.45651361, 0.41051704, 0.36823916]), '9': array([0.54027551, 0.46169773, 0.41215354, 0.36339167]), '10': array([0.5433206 , 0.46222359, 0.42310634, 0.38119945]), '11': array([0.78597027, 0.54787713, 0.52799845, 0.50952983])}, 'mean_train_accuracies_history': {'0': array([0.72405261, 0.79594737, 0.82378948, 0.85799998, 0.8881579 ]), '1': array([0.72247368, 0.79500002, 0.82068419, 0.8504737 , 0.88431579]), '2': array([0.7283684 , 0.79126316, 0.8134737 , 0.84721053]), '3': array([0.7136842 , 0.77931577, 0.79810524, 0.81036842, 0.82457894]), '4': array([0.72321051, 0.78763157, 0.81531578, 0.84573686]), '5': array([0.71978945, 0.78994739, 0.81773686, 0.84463155]), '6': array([0.72947371, 0.78610528, 0.81289476, 0.83463156]), '7': array([0.72399998, 0.78310525, 0.80621052, 0.8198421 , 0.83442104,\n",
      "       0.85426319, 0.86821055]), '8': array([0.71594739, 0.7854737 , 0.81194735, 0.83642107]), '9': array([0.72357893, 0.78378946, 0.81031579, 0.83805263]), '10': array([0.72542107, 0.78678948, 0.80726314, 0.8331579 ]), '11': array([0.71268421, 0.78584212, 0.80247366, 0.8176316 ])}, 'mean_val_losses_history': {'0': array([0.56370044, 0.54145348, 0.56468052, 0.60515904, 0.69444031]), '1': array([0.57323462, 0.52999401, 0.57812572, 0.62073916, 0.66154587]), '2': array([0.53196394, 0.53296459, 0.54105979, 0.54914719]), '3': array([0.61154944, 0.58864427, 0.60978758, 0.62311977, 0.64018971]), '4': array([0.51955229, 0.51965481, 0.54323775, 0.56405312]), '5': array([0.52743441, 0.52867186, 0.5436253 , 0.58287942]), '6': array([0.53545904, 0.53537363, 0.57428652, 0.57988018]), '7': array([0.60842234, 0.59523034, 0.60804433, 0.59263605, 0.60705662,\n",
      "       0.626001  , 0.62063134]), '8': array([0.53215188, 0.53140688, 0.57412183, 0.57971692]), '9': array([0.53375572, 0.53374803, 0.5591355 , 0.56754988]), '10': array([0.5195331 , 0.53033292, 0.55810386, 0.57080185]), '11': array([0.60199511, 0.6146577 , 0.62314451, 0.60854334])}, 'mean_val_accuracies_history': {'0': array([0.72000003, 0.73799998, 0.70599997, 0.708     , 0.72000003]), '1': array([0.72799999, 0.71600002, 0.70999998, 0.70999998, 0.70599997]), '2': array([0.736     , 0.71200001, 0.722     , 0.71799999]), '3': array([0.71799999, 0.73400003, 0.71799999, 0.704     , 0.72000003]), '4': array([0.74000001, 0.73400003, 0.736     , 0.73799998]), '5': array([0.72000003, 0.72399998, 0.72000003, 0.70999998]), '6': array([0.73000002, 0.73799998, 0.722     , 0.71399999]), '7': array([0.71200001, 0.73799998, 0.722     , 0.72399998, 0.73799998,\n",
      "       0.72399998, 0.74599999]), '8': array([0.72000003, 0.73400003, 0.708     , 0.72799999]), '9': array([0.73400003, 0.72600001, 0.71799999, 0.71399999]), '10': array([0.73199999, 0.74400002, 0.71799999, 0.71799999]), '11': array([0.72399998, 0.72600001, 0.704     , 0.736     ])}, 'best_mean_val_loss': 0.519533097743988, 'best_mean_train_accuracy': 0.7254210710525513, 'best_mean_val_accuracy': 0.7319999933242798, 'best_config': {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.01}, 'best_config_n': 10, 'best_epoch_n': 0}\n",
      "\n",
      "Save the training records into model_params\n",
      "\n",
      "****************************************\n",
      "Retrain the model under the best configuration\n",
      "****************************************\n",
      "Load the best configuration\n",
      "Retrain the model with that configuration\n",
      "\n",
      "Configuration 1 / 1: {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01, 'dropout_rate': 0.5, 'l2_reg': 0.01}\n",
      "Fold 1 / 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54115, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54115\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54115 to 0.53775, saving model to D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\checkpoints\\weights.best.hdf5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53775\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53775\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53775\n",
      "\n",
      " Currently best mean val_loss: 0.5377538800239563\n",
      "\n",
      " Currently best mean val_accuracy: 0.7300000190734863\n",
      "\n",
      "Load the model with the best metric over epochs\n",
      "Save the trained model in the model file\n",
      "\n",
      "WARNING:tensorflow:From D:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From D:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./models/model_cnn+lstm.p\\assets\n",
      "\n",
      "****************************************\n",
      "Compute the metric on the test set\n",
      "****************************************\n",
      "\n",
      "Evaluate the model on the train set (for comparisons)\n",
      "Loss = 0.3652264475822449, Accuracy = 0.8421579003334045, AUC = 0.927460789680481\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'log'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-aadaa7882a77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Run the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m simu.simu(None,\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpadded_corpus_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpadded_corpus_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\\src\\simu.py\u001b[0m in \u001b[0;36msimu\u001b[1;34m(run, padded_corpus_train, padded_corpus_val, padded_corpus_test, y_train, y_val, y_test, model_params)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Loss = {loss}, Accuracy = {accuracy}, AUC = {auc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_set_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEvaluate the model on the test set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'log'"
     ]
    }
   ],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(models)\n",
    "importlib.reload(simu)\n",
    "importlib.reload(simu_framework)\n",
    "\n",
    "# Run the simulation\n",
    "simu.simu(None,\n",
    "        padded_corpus_train,\n",
    "        padded_corpus_val,\n",
    "        padded_corpus_test,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        y_test,\n",
    "        model_params,\n",
    "        )\n",
    "\n",
    "# Save the model\n",
    "print(\"\\nDump the model parameters in a pickle file\")\n",
    "with open(MODEL_PARAMS_FILE, 'wb') as f:\n",
    "        pickle.dump(model_params, f)"
   ]
  },
  {
   "source": [
    "# Train the model on Azure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "importlib.reload(config)\n",
    "from azureml.core import Workspace, Experiment, Environment, Dataset, ScriptRunConfig\n",
    "\n",
    "# Name the model\n",
    "model_name = config.model_name\n",
    "\n",
    "# Load the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"\\nWorkspace loaded\")\n",
    "\n",
    "# Get the default Azure Machine Learning datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "print(\"\\nDatastore created\")\n",
    "\n",
    "# Instantiate an Azure Machine Learning dataset\n",
    "dataset = Dataset.File.from_files(path=[(datastore, 'datasets')])\n",
    "print(\"\\nDataset instantiated\")\n",
    "\n",
    "# Register the dataset\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='data',\n",
    "                           description='DonnÃ©es P7',\n",
    "                           create_new_version=True)\n",
    "print(\"\\nDataset registered\")\n",
    "\n",
    "# Define what compute target to use\n",
    "compute_target = ws.compute_targets['gpu-cluster']\n",
    "print(\"\\nCompute target defined\")\n",
    "\n",
    "# Configure an environment\n",
    "env = Environment.from_conda_specification(name='P7-env-gpu',\n",
    "                                           file_path='./.azureml/P7-env-gpu.yml')\n",
    "print(\"\\nEnvironment configured\")\n",
    "\n",
    "# Specify a GPU base image\n",
    "env.docker.enabled = True\n",
    "env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "print(\"\\nGPU image specified\")\n",
    "\n",
    "# Instantiate a script\n",
    "# Define the source directory, the main script, the compute target, the arguments\n",
    "src = ScriptRunConfig(source_directory='./src',\n",
    "                      script='train.py',\n",
    "                      compute_target=compute_target,\n",
    "                      arguments=[\n",
    "                                 '--data-folder', dataset.as_named_input('input').as_mount(),\n",
    "                                 '--model-name', model_name,\n",
    "                                ],\n",
    "                     )\n",
    "print(\"\\nScript configured\")\n",
    "\n",
    "# Update the script with the environment\n",
    "src.run_config.environment = env\n",
    "print(\"\\nConfig updated with the GPU image\")\n",
    "\n",
    "# Instantiate an experiment\n",
    "experiment = Experiment(workspace=ws, name='xp-1')\n",
    "print(\"\\nExperiment instantiated\")\n",
    "\n",
    "# Run an experiment\n",
    "run = experiment.submit(src)\n",
    "\n",
    "# Display the link to the experiment\n",
    "aml_url = run.get_portal_url()\n",
    "print(\"\\nSubmitted to compute cluster. Click link below\")\n",
    "print(\"\")\n",
    "print(aml_url)\n"
   ]
  },
  {
   "source": [
    "# Deploy the model on Azure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\"\n",
    "SRC_DIR = ROOT_DIR + r\"\\src\"\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "import config\n",
    "import pickle\n",
    "import importlib\n",
    "from azureml.core import Workspace, Environment, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\"\n",
    "SRC_DIR = ROOT_DIR + r\"\\src\"\n",
    "sys.path.append(SRC_DIR)\n",
    "import config\n",
    "\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "#Retrieve the model name\n",
    "model_name = config.model_name\n",
    "\n",
    "# Load the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"\\nWorkspace loaded\")\n",
    "\n",
    "# Clean-up\n",
    "# Delete services with the same name\n",
    "if config.model_name in ws.webservices:\n",
    "    ws.webservices[config.model_name].delete()\n",
    "print('\\nClean-up done')\n",
    "\n",
    "# Configure an environment\n",
    "env = Environment.from_conda_specification(name='P7-env',\n",
    "                                           file_path='./.azureml/P7-env.yml')\n",
    "print(\"\\nEnvironment configured\")\n",
    "\n",
    "# Combine the script and environment in an InferenceConfig\n",
    "inference_config = InferenceConfig(source_directory='./src',\n",
    "                                   entry_script='entry_script.py',\n",
    "                                   environment=env)\n",
    "print(\"\\nInference configured\") \n",
    "\n",
    "# Define what compute target to use\n",
    "compute_target = ws.compute_targets['cpu-cluster']\n",
    "print(\"\\nCompute target defined\")\n",
    "\n",
    "# Define the deployment configuration which sets the target \n",
    "# specific compute specification for the containerized deployement\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "print(\"\\nDeployment configured\\n\")\n",
    "\n",
    "# Deploy the model\n",
    "model = ws.models[model_name]\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = model_name,\n",
    "                       models = [model],\n",
    "                       inference_config = inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       deployment_target = None)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print('\\nDeployment done')"
   ]
  },
  {
   "source": [
    "# Consume the web service on Azure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Test the run(data) script locally"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = r\"D:\\Data\\Google Drive\\Openclassrooms\\P7\\Projet\"\n",
    "SRC_DIR = ROOT_DIR + r\"\\src\"\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import load\n",
    "import preprocess\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(\"\\n****************************************\")\n",
    "print(\"SEND REQUEST\")\n",
    "print(\"****************************************\")\n",
    "\n",
    "# Set-up Data\n",
    "data = {\"data\":\n",
    "        [\n",
    "           \"I hate this company !\",\n",
    "           \"The flight was great\"\n",
    "        ]\n",
    "        }\n",
    "print(\"\\nData set-up\")\n",
    "\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "print('\\nConvert to JSON')\n",
    "\n",
    "\n",
    "print(\"\\n****************************************\")\n",
    "print(\"TREAT REQUEST\")\n",
    "print(\"****************************************\")\n",
    "\n",
    "# Set up  the parameters of the model\n",
    "model_params['train_test_split']['n_samples'] = 10\n",
    "model_params['train_test_split']['test_size'] = 1.0\n",
    "model_params['train_test_split']['val_test_ratio'] = 0.0\n",
    "print('\\nParameters of the model set-up')\n",
    "\n",
    "# Load the data\n",
    "corpus_test = json.loads(input_data)['data']\n",
    "y_test = np.zeros(len(corpus_test))\n",
    "print('\\nData loaded')\n",
    "\n",
    "# Load the model\n",
    "model_name = config.model_name\n",
    "model_params_file_name = f\"./models/model_params_{model_name}.p\"\n",
    "model_file_name = f\"./models/model_{model_name}.p\"\n",
    "model = load_model(model_file_name)\n",
    "print('\\nModel loaded')\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessing_inputs = (corpus_test, y_test, model_params)\n",
    "padded_corpus_val, padded_corpus_test, y_val, y_test = preprocess.preprocess_test(*preprocessing_inputs)\n",
    "print('\\nData preprocessed')\n",
    "\n",
    "# Predict the data\n",
    "result = model.predict(padded_corpus_test)\n",
    "result = [\"positive\" if r > 0.5 else \"negative\" for r in result]\n",
    "result = zip(result, corpus_test)\n",
    "print(f'\\nRequest result: {list(result)}')"
   ]
  },
  {
   "source": [
    "## Test the call to the webservice (on Azure)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data', type=list, dest='data', help='List of tweets')\n",
    "# args = parser.parse_args()\n",
    "# data = args.data\n",
    "\n",
    "# Format the data\n",
    "data = {\"data\":\n",
    "        [\n",
    "           \"I hate this company !\",\n",
    "           \"The flight was great\"\n",
    "        ]\n",
    "        }\n",
    "\n",
    "input_data = json.dumps(data)\n",
    "print(\"\\nData formatted\")\n",
    "\n",
    "# Connect to workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"\\nWorkspace connected\")\n",
    "\n",
    "# Connect to webservice\n",
    "webservice = Webservice(workspace=ws, name='model-1')\n",
    "scoring_uri = webservice.scoring_uri\n",
    "print(\"\\nWebservice connected\")\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "# If authentication is enabled, set the authorization header\n",
    "# headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, input_data, headers=headers)\n",
    "print('\\nRequest done')\n",
    "\n",
    "print('\\nResponse:')\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}